# Incident 1 

Open LND issue  https://github.com/lightningnetwork/lnd/issues/3370 open since Aug 2019.

Not a recent regression - I’ve been noticing this problem for about 1 year. Restarting LND makes it go away for days or weeks.

LND go seem to be stuck in GC / Malloc for the ZMQ.


["version": "0.9.0-beta commit=v0.9.0-beta-68-g2cd26d75564345c0c37fa83785ebe8e4f1fb8f28"](https://github.com/lightningnetwork/lnd/commit/2cd26d75564345c0c37fa83785ebe8e4f1fb8f28) Feb 4, 2020

# Table of Contents

* [Impact](https://github.com/alevchuk/minibank/blob/master/incidents/i1.md#impact)
* [Symptoms](https://github.com/alevchuk/minibank/blob/master/incidents/i1.md#symptoms)
* [Current plan](https://github.com/alevchuk/minibank/blob/master/incidents/i1.md#current-plan)
* [2020-04-21 03:41:00 UTC](https://github.com/alevchuk/minibank/blob/master/incidents/i1.md#2020-04-21-034100-utc)
* [2020-02-20 14:00:00 UTC](https://github.com/alevchuk/minibank/blob/master/incidents/i1.md#2020-02-20-140000-utc)
* [2020-02-18 01:48:00 UTC](https://github.com/alevchuk/minibank/blob/master/incidents/i1.md#2020-02-18-014800-utc)
* [2020-02-15 05:34:00 UTC](https://github.com/alevchuk/minibank/blob/master/incidents/i1.md#2020-02-15-053400-utc)
* [2020-02-09 02:00:00 UTC](https://github.com/alevchuk/minibank/blob/master/incidents/i1.md#2020-02-09-020000-utc)
* [Actions Taken](https://github.com/alevchuk/minibank/blob/master/incidents/i1.md#actions-taken)

## Impact

CPU at 100%. In this state, sometimes LND times out when trying to generate or list invoices over RPC.

For example, `addinvoice` takes 42 seconds. Usually, I force timeout `addinvoice` RPC at 5-10 seconds, yet if I wait long enough and let the call succeed then consecutive `addinvoice` calls are fast again. Yet, now routing to this node fails when I try to pay the invoice from a different node.


## Symptoms

![cpu](/incidents/i1/i1-symptom.png)

## Current plan

**update apr 16**: Auto remediate https://github.com/alevchuk/minibank/blob/master/scripts/REMEDIATION.md


## 2020-04-21 03:41:00 UTC

I recorded one just now with `perf record -F 99 -a -p <PID> -a sleep 60` while the issue is happening.

Here it is: https://github.com/alevchuk/minibank/blob/master/incidents/i1/perf.data

same with call graph `perf record -F 99 -ag -p <PID> -a sleep 60`

Here it is: https://github.com/alevchuk/minibank/blob/master/incidents/i1/perf_with_call_graph.data

(i don't have privacy concerns about this node, so posting publicly)

posted this on the gituhub issue


## 2020-04-15
updgrade `go` so that LND runs the latest GC imporvemenet, that did not help


## 2020-02-20
reduce logging - turn off bitcoind debug logs, that did not help


## 2020-02-20 14:00:00 UTC
* bl3
* 4 GB ram
* 2 CPU core

Increased vm size from t2.small (1 cpu, 2gb ram) to t2.medium (2 cpu, 4gb ram).

The gc issues are still happening.

![cpu](/incidents/i1/i1-gc-feb20.png)


## 2020-02-18 01:48:00 UTC

Now it's also happening on a node of same config yet without btcoind - i'm using a remote full node.

```
sudo perf record -F 99 -a -p 18744 -a sleep 5
sudo perf report

```

![gc](/incidents/i1/i1-on-cpu-2.png)
  

## 2020-02-15 05:34:00 UTC

This time I had no Swap configured. Just plain 2GB of RAM.
Looks same as in previous times


* on-CPU functions
  * `sudo perf record -F 99 -a -p 18744 -a sleep 5`
  * ![on-CPU](/incidents/i1/i1-on-cpu.png)

* call-graph (stack chain/backtrace) recording (`sudo perf record -F 99 -ag -p 18744 -a sleep 5`)
  * ![call-graph](/incidents/i1/i1-call-graph.png)




## 2020-02-09 02:00:00 UTC
* bl3
* 2 GB ram
* 1 CPU core

At 8:30 UTC some of the User CPU is replaced by Steal CPU. Probably due to AWS hypervisor.

```
2020-02-09 02:01:09.504 New block: height=616582, sha=0000000000000000000e8243f6009497933f57dbc5aad956528418cd3e7cb341
Perf stat
          1,609.54 msec cpu-clock                 #    0.322 CPUs utilized
             3,052      context-switches          # 1896.830 M/sec
                 0      cpu-migrations            #    0.000 K/sec
             1,016      page-faults               #  631.448 M/sec
```

LND go seem to be stuck in GC:
```
sudo perf record -F 99  -p 8329  -a sleep 5
sudo perf report
```
  * ![flame-graph-1](/incidents/i1/i1-perf-kernel-incident.svg)
    * for a clickable SVG flame graph, download and open in your web-browser `curl https://raw.githubusercontent.com/alevchuk/minibank/master/incidents/i1-perf-kernel-incident.svg > i1.svg`
    * this SVG was generated like this http://www.brendangregg.com/perf.html#FlameGraphs

### atop ###
```
atop -r  /var/log/atop/atop_20200209
```

Everything looks normal, except CPU goes up for lnd at 2:00

* Normal swap in/out activity
* Normal I/O
* Normal memory

### vmstat ###

Everything looks normal, except lot of user level CPU.

I initially had 600 MB swap, yet later turned it off - there was no effect.
```
vmstat 60
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 2  0 268960  79412    880 866496    0   10  5303   744  240  678 30  1  0  4 65
 1  0 269592 106500    852 841224    1   11  1731   791  278  694 45  1  0  3 51
 2  0 269536  77964   4784 867516    2    3  2592   474  305  746 51  1  0  1 47
 2  0 269912  78556   4688 860768    6   11  4720   688  218  437 30  1  0  3 66
 1  0 270740  73320   4456 872868    0   14  4180   477  196  547 31  1  0  2 66
 1  1 254124  80680   4452 850024  280    0  2425   389  229  636 31  1  0  1 67
 2  0 223040  82884   4468 819448  504    2  4593  1031  390  855 46  1  0  1 51
 4  0 207124  70064   4448 818868  263    8  6889   401  406  997 49  2  0  2 48
```

Same after swap was turned off and when incident was happening again

```
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 2  0      0  72604    632 620624    0    0  4056  8568 1056 2292 44  3  0 53  0
 1  0      0  67848    640 625300    0    0  1441   143  452  553 99  0  0  1  0
 1  0      0 101772    632 591528    0    0   852  8809  668 1245 96  2  0  2  0
 1  0      0  77336    632 615592    0    0  7133   357  505  596 98  1  0  1  0
 1  0      0  69272    640 623808    0    0  2739    19  365  448 78  0  0  0 22
 2  0      0  68516    640 624476    0    0    79   120  316  465 84  0  0  1 15
 1  2      0  84352    624 607536    0    0  7065  1569  588 1097 30  1  0 32 37
 3  0      0  71280    624 621740    0    0  3585   907  298  582 64  2  0  5 30
 2  0      0  77836    624 613868    0    0  9392     1  157  322 21  1  0  0 78
 2  0      0  72828    624 620140    0    0  2085     0  175  325 73  1  0  0 26
 3  0      0  72828    624 620140    0    0     8     0  147  242 99  1  0  0  0
 3  0      0  87768    624 605292    0    0  3483     0  157  262 15  0  0  0 85
```

# Actions Taken

* Turned off swap → no effect
* Restart LND  → mitigated the issue for days or weeks.
* channel-cache-size mitigation - no effect
* added more RAM - did not help, details: [2020-02-20 14:00:00 UTC](https://github.com/alevchuk/minibank/blob/master/incidents/i1.md#2020-02-20-140000-utc)

### channel-cache-size mitigation attempt

**feb 18 update: this plan did not help**

To mitigate, I want to try to set
> caches.channel-cache-size to 35000, rather than the default of 20000

as suggested in the LND issue.




### Restart LND mitigation

Mitigates the problem.

Nothing interesting in logs (I don't spot any differences) at the time the incident starts or during shutdown.

LND github issue mentions looking "applying new update horizon:" with backlog size of 50k+ yet my logs do not exceed 22:

```
grep "applying new update horizon:" .lnd/logs/bitcoin/mainnet/lnd.log | grep -o "backlog_size=.*" | sort | uniq  -c
    255 backlog_size=0
      3 backlog_size=1
      1 backlog_size=10
      1 backlog_size=11
      1 backlog_size=12
      4 backlog_size=2
      1 backlog_size=22
     37 backlog_size=3
      4 backlog_size=4
      1 backlog_size=5
     10 backlog_size=6
      1 backlog_size=7
      8 backlog_size=9
```

This is most likely not related -> on shutdown
```
reason: server: disconnecting peer
PEER: unable to read message from XYZ read tcp 127.0.0.1:42058->127.0.0.1:9050: use of closed network connection
```
